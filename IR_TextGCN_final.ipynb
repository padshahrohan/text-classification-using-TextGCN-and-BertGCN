{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgeXRA4Vd3OO"
      },
      "source": [
        "# Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgR2TwFZ6QIX",
        "outputId": "d8e0255f-7691-4f2b-b1fb-38c2754ca834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "WNYmVQlhd8wo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "61cfd551-51a5-4d79-b7eb-7a0bd5f6293c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       Article title  \\\n",
              "0  Assessing the impacts of COVID-19 vaccination ...   \n",
              "1  Association between interleukin-10 gene polymo...   \n",
              "2  Quality of Life of early-stage breast-cancer p...   \n",
              "3  The research interest, capacity and culture of...   \n",
              "4  Machine learning prediction for COVID-19 disea...   \n",
              "\n",
              "                                    Article keywords  \\\n",
              "0  Affordability;COVID-19 | SARS-CoV-2;Decision-m...   \n",
              "1  COVID-19;Interleukin-10 gene polymorphisms;SAR...   \n",
              "2  Breast;COVID-19;Cancer;EORTC;Oncology;Quality ...   \n",
              "3  Barriers;Health Research;Innovation;Motivators...   \n",
              "4  COVID-19;Classification;Laboratory markers;Mac...   \n",
              "\n",
              "                                    Article abstract  Contextual  \n",
              "0  The COVID-19 vaccine supply shortage in 2021 c...         1.0  \n",
              "1  Polymorphisms in the interleukin-10 (IL10) gen...         1.0  \n",
              "2  Objectives To describe the Quality of Life (QO...         1.0  \n",
              "3  The UK National Health Service (NHS) is ideall...         0.0  \n",
              "4  Early prognostication of patients hospitalized...         1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47734e1a-587f-4b97-ba21-7ae8a2ce5f9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article title</th>\n",
              "      <th>Article keywords</th>\n",
              "      <th>Article abstract</th>\n",
              "      <th>Contextual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Assessing the impacts of COVID-19 vaccination ...</td>\n",
              "      <td>Affordability;COVID-19 | SARS-CoV-2;Decision-m...</td>\n",
              "      <td>The COVID-19 vaccine supply shortage in 2021 c...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Association between interleukin-10 gene polymo...</td>\n",
              "      <td>COVID-19;Interleukin-10 gene polymorphisms;SAR...</td>\n",
              "      <td>Polymorphisms in the interleukin-10 (IL10) gen...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Quality of Life of early-stage breast-cancer p...</td>\n",
              "      <td>Breast;COVID-19;Cancer;EORTC;Oncology;Quality ...</td>\n",
              "      <td>Objectives To describe the Quality of Life (QO...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The research interest, capacity and culture of...</td>\n",
              "      <td>Barriers;Health Research;Innovation;Motivators...</td>\n",
              "      <td>The UK National Health Service (NHS) is ideall...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Machine learning prediction for COVID-19 disea...</td>\n",
              "      <td>COVID-19;Classification;Laboratory markers;Mac...</td>\n",
              "      <td>Early prognostication of patients hospitalized...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47734e1a-587f-4b97-ba21-7ae8a2ce5f9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47734e1a-587f-4b97-ba21-7ae8a2ce5f9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47734e1a-587f-4b97-ba21-7ae8a2ce5f9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/PubMed_records_for_covid-19_labelled&unlabelled.xlsx - Sheet1 (1).csv\")\n",
        "\n",
        "df = df[['Article title', 'Article keywords' ,'Article abstract', 'Contextual']].copy()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "yPyHpNI0fV1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5dfb0e-564a-42aa-9ae5-4340021ba148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "Hs6MJiW1jJ4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db5de93-a33a-44cb-c079-209ef5a3dbce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "print(stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "MoNjcv2kjx5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccbdc19-208f-41e3-860d-174ec66da3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-198-cfda384d0022>:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Article title'] = df['Article title'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
            "<ipython-input-198-cfda384d0022>:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Article abstract'] = df['Article abstract'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
            "<ipython-input-198-cfda384d0022>:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Article keywords'] = df['Article keywords'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "df['Article title'] = df['Article title'].str.lower()\n",
        "df['Article keywords'] = df['Article keywords'].str.lower()\n",
        "df['Article abstract'] = df['Article abstract'].str.lower()\n",
        "\n",
        "#If accuracy is bad then try removing on relevant punctuations after assessing the data\n",
        "df['Article title'] = df['Article title'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
        "df['Article abstract'] = df['Article abstract'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
        "df['Article keywords'] = df['Article keywords'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
        "\n",
        "df['Article title'] = df['Article title'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "df['Article keywords'] = df['Article keywords'].apply(lambda x: ' '.join([item for item in str(x).split(';') if item not in stop]))\n",
        "df['Article abstract'] = df['Article abstract'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "WxDhqnkBkkVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa66fa89-49fb-46cc-badf-1a02bd0c998a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 76, 77, 80, 81, 82, 83, 84, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]\n",
            "[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 8833 entries, 0 to 8832\n",
            "Data columns (total 4 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Article title     8833 non-null   object \n",
            " 1   Article keywords  8833 non-null   object \n",
            " 2   Article abstract  8833 non-null   object \n",
            " 3   Contextual        131 non-null    float64\n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 345.0+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 57 entries, 145 to 206\n",
            "Data columns (total 4 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Article title     57 non-null     object \n",
            " 1   Article keywords  57 non-null     object \n",
            " 2   Article abstract  57 non-null     object \n",
            " 3   Contextual        57 non-null     float64\n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 2.2+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_labeled = df[df['Contextual'].notnull()]\n",
        "df_unlabeled = df[df['Contextual'].isnull()]\n",
        "df_train, df_test = train_test_split(df_labeled, test_size=0.2, shuffle=False)\n",
        "\n",
        "labeled_index_in_train = list(df_train.index.values)\n",
        "labeled_index_in_test = list(df_test.index.values)\n",
        "unlabeled_index = list(df_unlabeled.index.values)\n",
        "\n",
        "train_size = len(df_train) + len(df_test)  + len(df_unlabeled)\n",
        "\n",
        "df_test_without_contextual = df_test[['Article title', 'Article keywords', 'Article abstract']]\n",
        "\n",
        "df_train = pd.concat([df_train, df_test_without_contextual, df_unlabeled])\n",
        "\n",
        "# print(df_train.head())\n",
        "# print(df_test.head())\n",
        "\n",
        "print(labeled_index_in_train)\n",
        "print(labeled_index_in_test)\n",
        "\n",
        "print(df_train.info())\n",
        "print(df_test.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvslpG2Vmsck"
      },
      "source": [
        "# Build Graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQoGU2dLibzY"
      },
      "source": [
        "## Build list of training documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "_KuUv21xUooH"
      },
      "outputs": [],
      "source": [
        "row = []\n",
        "col = []\n",
        "weight = []\n",
        "\n",
        "train_list = []\n",
        "\n",
        "for index in df_train.index:\n",
        "  train_list.append( str(index) + ':=:' + df_train['Article title'][index] + df_train['Article keywords'][index] + df_train['Article abstract'][index])  # \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkkiiQ_V3yc1"
      },
      "source": [
        "## TF-IDF for document-word weight in graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "_fX51_3AkuOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8f1f75-9a06-4969-d94d-52d4dd607c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "636812\n",
            "636812\n",
            "636812\n"
          ]
        }
      ],
      "source": [
        "from math import log\n",
        "# doc word frequency\n",
        "# TF is simple raw frequency unlike (1+log(tf)), we might want to change that if accuracy is not good\n",
        "\n",
        "# build vocab\n",
        "word_set = set()\n",
        "for doc_words in train_list:\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    for word in words:\n",
        "        word_set.add(word)\n",
        "\n",
        "vocab = list(word_set)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_id_map = {}\n",
        "for i in range(vocab_size):\n",
        "    word_id_map[vocab[i]] = i\n",
        "\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(train_size):\n",
        "    doc_words = train_list[doc_id]\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1\n",
        "\n",
        "\n",
        "word_doc_list = {}\n",
        "\n",
        "for i in range(train_size):\n",
        "    doc_words = train_list[i]\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    appeared = set()\n",
        "    for word in words:\n",
        "        if word in appeared:\n",
        "            continue\n",
        "        if word in word_doc_list:\n",
        "            doc_list = word_doc_list[word]\n",
        "            doc_list.append(i)\n",
        "            word_doc_list[word] = doc_list\n",
        "        else:\n",
        "            word_doc_list[word] = [i]\n",
        "        appeared.add(word)\n",
        "\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "for i in range(train_size):\n",
        "    doc_words = train_list[i]\n",
        "    # words = doc_words.split()\n",
        "    row_index = int(doc_words.split(':=:')[0])\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        # row.append(i)\n",
        "        row.append(row_index)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(train_list) /\n",
        "                  word_doc_freq[vocab[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)\n",
        "\n",
        "print(len(row))\n",
        "print(len(col))\n",
        "print(len(weight))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69odkc3-yl84"
      },
      "source": [
        "## PMI Calculation for word-word edge weight in graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "rurMveZjoL7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b188041-df79-4b37-fb77-b5a84ae237e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11435398\n",
            "11435398\n",
            "11435398\n"
          ]
        }
      ],
      "source": [
        "#Get the train as a list and define window size\n",
        "window_size = 20\n",
        "windows = []\n",
        "\n",
        "for doc_words in train_list:\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    length = len(words)\n",
        "    if length <= window_size:\n",
        "        windows.append(words)\n",
        "    else:\n",
        "        for j in range(length - window_size + 1):\n",
        "            window = words[j: j + window_size]\n",
        "            windows.append(window)\n",
        "\n",
        "# print(windows)\n",
        "\n",
        "word_window_freq = {}\n",
        "for window in windows:\n",
        "    appeared = set()\n",
        "    for i in range(len(window)):\n",
        "        if window[i] in appeared:\n",
        "            continue\n",
        "        if window[i] in word_window_freq:\n",
        "            word_window_freq[window[i]] += 1\n",
        "        else:\n",
        "            word_window_freq[window[i]] = 1\n",
        "        appeared.add(window[i])\n",
        "\n",
        "# print(word_window_freq)\n",
        "\n",
        "word_pair_count = {}\n",
        "for window in windows:\n",
        "    for i in range(1, len(window)):\n",
        "        for j in range(0, i):\n",
        "            word_i = window[i]\n",
        "            word_i_id = word_id_map[word_i]\n",
        "            word_j = window[j]\n",
        "            word_j_id = word_id_map[word_j]\n",
        "            if word_i_id == word_j_id:\n",
        "                continue\n",
        "            # print(word_i + ' ' + word_j + ' ' + str(word_i_id) + ' ' + str(word_j_id))\n",
        "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "            # two orders\n",
        "            # print(word_j + ' ' + word_i + ' ' + str(word_j_id) + ' ' + str(word_i_id))\n",
        "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "\n",
        "\n",
        "# print(word_pair_count)\n",
        "\n",
        "# pmi as weights\n",
        "\n",
        "num_window = len(windows)\n",
        "\n",
        "for key in word_pair_count:\n",
        "    temp = key.split(',')\n",
        "    i = int(temp[0])\n",
        "    j = int(temp[1])\n",
        "    count = word_pair_count[key]\n",
        "    word_freq_i = word_window_freq[vocab[i]]\n",
        "    word_freq_j = word_window_freq[vocab[j]]\n",
        "    pmi = log((1.0 * count / num_window) /\n",
        "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
        "    if pmi <= 0:\n",
        "        continue\n",
        "    #Adjust the position of pmi weights in final adjacency matrix\n",
        "    row.append(train_size + i)\n",
        "    col.append(train_size + j)\n",
        "    weight.append(pmi)\n",
        "\n",
        "print(len(row))\n",
        "print(len(col))\n",
        "print(len(weight))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjN00HkjUiXG"
      },
      "source": [
        "## Diagonal edge weight initialized to 1 in the adjacency matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "WcDQOk8vigxE"
      },
      "outputs": [],
      "source": [
        "node_size = train_size + vocab_size\n",
        "\n",
        "for i in range(node_size):\n",
        "  row.append(i)\n",
        "  col.append(i)\n",
        "  weight.append(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jynF9NAC5M8o"
      },
      "source": [
        "## Adjacency Matrix : A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "2bJPpbyPxFAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b5011e-6e00-48be-c336-228fd49bd245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11520811\n",
            "11520811\n",
            "11520811\n",
            "85413\n",
            "  (0, 0)\t1.0\n",
            "  (0, 9298)\t3.6525279831908923\n",
            "  (0, 10758)\t4.768761873208821\n",
            "  (0, 14637)\t4.347014332503912\n",
            "  (0, 14856)\t3.5728212405801503\n",
            "  (0, 15110)\t4.137490096366964\n",
            "  (0, 15357)\t9.086249986745132\n",
            "  (0, 20246)\t4.5115390082417495\n",
            "  (0, 20807)\t3.502753677963433\n",
            "  (0, 23773)\t3.426767770985511\n",
            "  (0, 27594)\t2.968152788703784\n",
            "  (0, 28597)\t3.426767770985511\n",
            "  (0, 30297)\t5.397370532631196\n",
            "  (0, 30500)\t2.213086152532614\n",
            "  (0, 31003)\t6.041727549021709\n",
            "  (0, 31808)\t8.393102806185187\n",
            "  (0, 34890)\t2.559755127174342\n",
            "  (0, 36022)\t4.837754744695773\n",
            "  (0, 36190)\t7.140339837689819\n",
            "  (0, 39154)\t3.2993526053784246\n",
            "  (0, 39197)\t3.34324679893565\n",
            "  (0, 39658)\t0.23287031381749881\n",
            "  (0, 41124)\t9.086249986745132\n",
            "  (0, 41339)\t5.060898296009983\n",
            "  (0, 42915)\t5.530901925255718\n",
            "  :\t:\n",
            "  (85411, 84936)\t3.116633014507707\n",
            "  (85411, 85254)\t0.9595787196986144\n",
            "  (85411, 85411)\t1.0\n",
            "  (85412, 19)\t9.086249986745132\n",
            "  (85412, 10532)\t4.713793646043373\n",
            "  (85412, 14658)\t4.873122838816369\n",
            "  (85412, 24178)\t7.2464329508054846\n",
            "  (85412, 27549)\t7.612495511047619\n",
            "  (85412, 27960)\t9.381782124423616\n",
            "  (85412, 28694)\t1.6684045183383\n",
            "  (85412, 36120)\t6.755906531510422\n",
            "  (85412, 38172)\t10.991220036857717\n",
            "  (85412, 39658)\t0.9430502990682924\n",
            "  (85412, 45880)\t6.058546284231173\n",
            "  (85412, 46167)\t10.991220036857717\n",
            "  (85412, 47911)\t4.225412420531261\n",
            "  (85412, 49388)\t10.991220036857717\n",
            "  (85412, 59542)\t4.641207301429322\n",
            "  (85412, 61160)\t3.5575042465253186\n",
            "  (85412, 62029)\t6.510479929247802\n",
            "  (85412, 71436)\t2.72755801459223\n",
            "  (85412, 76651)\t9.119417859956124\n",
            "  (85412, 82472)\t10.991220036857717\n",
            "  (85412, 82549)\t3.674422721277072\n",
            "  (85412, 85412)\t1.0\n"
          ]
        }
      ],
      "source": [
        "import scipy.sparse as sp\n",
        "\n",
        "#Train size contains all data (train + test both labeled and unlabeled)\n",
        "print(len(row))\n",
        "print(len(col)) \n",
        "print(len(weight))\n",
        "print(node_size)\n",
        "\n",
        "adj = sp.csr_matrix(\n",
        "    (weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "print(adj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnMhaEc3zdPW"
      },
      "source": [
        "## Build Feature Matrix: X "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "hhZz_jQubpuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d68fda-0207-4080-8c14-2de3f95399eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(85413, 85413)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "row_x = []\n",
        "col_x = []\n",
        "weight_x = []\n",
        "\n",
        "# One hot vector for X as per text GCN\n",
        "for i in range(node_size):\n",
        "  row_x.append(i)\n",
        "  col_x.append(i)\n",
        "  weight_x.append(1)\n",
        "\n",
        "x = sp.csr_matrix(\n",
        "    (weight_x, (row_x, col_x)), shape=(node_size, node_size))\n",
        "\n",
        "print(x.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCKWIz7MTkBy"
      },
      "source": [
        "## Build label matrix: Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "ciVH58UV_VxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed84ba2-92e0-46f1-813b-d5ecbf5b0ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(85413, 85413)\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 76, 77, 80, 81, 82, 83, 84, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]\n",
            "[145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206]\n",
            "[ True  True  True ... False False False]\n",
            "[False False False ... False False False]\n",
            "(85413, 2)\n",
            "(85413, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-207-526c66212c57>:38: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return np.array(mask, dtype=np.bool)\n"
          ]
        }
      ],
      "source": [
        "y = []\n",
        "ty = []\n",
        "label_list = [0,1]\n",
        "\n",
        "for index in df_train.index:\n",
        "  if index in labeled_index_in_train:\n",
        "    label = int(df_train['Contextual'][index])\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    y.append(one_hot)\n",
        "    ty.append([0,0])\n",
        "  elif index in labeled_index_in_test:\n",
        "    label = int(df_test['Contextual'][index])\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    ty.append(one_hot)\n",
        "    y.append([0,0])\n",
        "  else:\n",
        "    y.append([0,0])\n",
        "    ty.append([0,0])\n",
        "\n",
        "for i in range(vocab_size):\n",
        "  y.append([0,0])\n",
        "  ty.append([0,0])\n",
        "\n",
        "y = np.array(y)\n",
        "ty = np.array(ty)\n",
        "\n",
        "# print(x)\n",
        "# print(y[:200])\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "features = sp.vstack((x)).tolil()\n",
        "features = sp.identity(features.shape[0])  \n",
        "print(features.shape)\n",
        "\n",
        "idx_train = labeled_index_in_train\n",
        "idx_test = labeled_index_in_test\n",
        "\n",
        "print(idx_train)\n",
        "print(idx_test)\n",
        "\n",
        "train_mask = sample_mask(idx_train, y.shape[0])\n",
        "test_mask = sample_mask(idx_test, ty.shape[0])\n",
        "print(train_mask)\n",
        "print(test_mask)\n",
        "\n",
        "y_train = np.zeros(y.shape)\n",
        "y_test = np.zeros(ty.shape)\n",
        "\n",
        "y_train[train_mask, :] = y[train_mask, :]\n",
        "y_test[test_mask, :] = ty[test_mask, :]\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VLofG4ojQfs"
      },
      "source": [
        "# Create GCN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O037NXsZ6cgf"
      },
      "source": [
        "## Define Flags for tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "1BLQiNkejU9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05b232a-6ab7-418d-e0be-aaa5a5a50bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "from sklearn import metrics\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "del_all_flags(FLAGS)\n",
        "\n",
        "#Define flags\n",
        "flags.DEFINE_string('dataset', 'pubmed', 'Dataset string.')\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
        "flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 12, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 500, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 0,\n",
        "                   'Weight for L2 loss on embedding matrix.')  # 5e-4\n",
        "flags.DEFINE_integer('early_stopping', 10,\n",
        "                     'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2yhWZYd6gNy"
      },
      "source": [
        "## Define Layer classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "pl_Ry4bsznc2"
      },
      "outputs": [],
      "source": [
        "def zeros(shape, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros(shape, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "def glorot(shape, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
        "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def sparse_dropout(x, keep_prob, noise_shape):\n",
        "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    Implementation inspired by keras (http://keras.io).\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "        _log_vars(): Log all variables\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.sparse_inputs = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            if self.logging and not self.sparse_inputs:\n",
        "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
        "            outputs = self._call(inputs)\n",
        "            if self.logging:\n",
        "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
        "            return outputs\n",
        "\n",
        "    def _log_vars(self):\n",
        "        for var in self.vars:\n",
        "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
        "\n",
        "\n",
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Graph convolution layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
        "                 featureless=False, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.support = placeholders['support']\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            for i in range(len(self.support)):\n",
        "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
        "                                                        name='weights_' + str(i))\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # convolve\n",
        "        supports = list()\n",
        "        for i in range(len(self.support)):\n",
        "            if not self.featureless:\n",
        "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
        "                              sparse=self.sparse_inputs)\n",
        "            else:\n",
        "                pre_sup = self.vars['weights_' + str(i)]\n",
        "            support = dot(self.support[i], pre_sup, sparse=True)\n",
        "            supports.append(support)\n",
        "        output = tf.add_n(supports)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "        self.embedding = output #output\n",
        "        return self.act(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSBSpuhx6k0M"
      },
      "source": [
        "## Define Model classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "NbXhYjZ8jo0l"
      },
      "outputs": [],
      "source": [
        "def masked_softmax_cross_entropy(preds, labels, mask):\n",
        "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
        "    print(preds)\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def masked_accuracy(preds, labels, mask):\n",
        "    \"\"\"Accuracy with masking.\"\"\"\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
        "\n",
        "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    accuracy_all *= mask\n",
        "    return tf.reduce_mean(accuracy_all)\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "        self.placeholders = {}\n",
        "\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "\n",
        "        self.loss = 0\n",
        "        self.accuracy = 0\n",
        "        self.optimizer = None\n",
        "        self.opt_op = None\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "\n",
        "        # Build sequential layer model\n",
        "        self.activations.append(self.inputs)\n",
        "        for layer in self.layers:\n",
        "            hidden = layer(self.activations[-1])\n",
        "            self.activations.append(hidden)\n",
        "        self.outputs = self.activations[-1]\n",
        "\n",
        "        # Store model variables for easy access\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "        # Build metrics\n",
        "        self._loss()\n",
        "        self._accuracy()\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.loss)\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "    def _loss(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _accuracy(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, sess=None):\n",
        "        if not sess:\n",
        "            raise AttributeError(\"TensorFlow session not provided.\")\n",
        "        saver = tf.train.Saver(self.vars)\n",
        "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
        "        print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "    def load(self, sess=None):\n",
        "        if not sess:\n",
        "            raise AttributeError(\"TensorFlow session not provided.\")\n",
        "        saver = tf.train.Saver(self.vars)\n",
        "        save_path = \"tmp/%s.ckpt\" % self.name\n",
        "        saver.restore(sess, save_path)\n",
        "        print(\"Model restored from file: %s\" % save_path)\n",
        "\n",
        "\n",
        "class GCN(Model):\n",
        "    def __init__(self, placeholders, input_dim, **kwargs):\n",
        "        super(GCN, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = input_dim\n",
        "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
        "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def _loss(self):\n",
        "        # Weight decay loss\n",
        "        for var in self.layers[0].vars.values():\n",
        "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "\n",
        "        # Cross entropy error\n",
        "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
        "                                                  self.placeholders['labels_mask'])\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "        self.pred = tf.argmax(self.outputs, 1)\n",
        "        self.labels = tf.argmax(self.placeholders['labels'], 1)\n",
        "\n",
        "    def _build(self):\n",
        "\n",
        "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
        "                                            output_dim=FLAGS.hidden1,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,\n",
        "                                            dropout=True,\n",
        "                                            featureless=True,\n",
        "                                            sparse_inputs=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
        "                                            output_dim=self.output_dim,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=lambda x: x, #\n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3FyrMUe2Rp9"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "U-HE1zcZyiwa"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "seed = random.randint(1, 200)\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "myDGnHYw2gLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da095fd-71db-4031-9e0e-935d94f45cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(array([[    0,     0],\n",
            "       [ 9298,     0],\n",
            "       [10758,     0],\n",
            "       ...,\n",
            "       [82472, 85412],\n",
            "       [82549, 85412],\n",
            "       [85412, 85412]], dtype=int32), array([0.0089806 , 0.003025  , 0.00497513, ..., 0.07958011, 0.00289949,\n",
            "       0.01441331]), (85413, 85413))]\n"
          ]
        }
      ],
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "  \n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "features = preprocess_features(features)\n",
        "support = [preprocess_adj(adj)]\n",
        "\n",
        "num_supports = 1\n",
        "model_func = GCN\n",
        "\n",
        "print(support)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "QOMID3GT4uQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01db39d-519c-414b-e5ad-60b1fa93a08f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'support': [SparseTensor(indices=Tensor(\"Placeholder_66:0\", shape=(?, ?), dtype=int64), values=Tensor(\"Placeholder_65:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"Placeholder_64:0\", shape=(?,), dtype=int64))], 'features': SparseTensor(indices=Tensor(\"Placeholder_68:0\", shape=(?, 2), dtype=int64), values=Tensor(\"Placeholder_67:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"PlaceholderWithDefault_16:0\", shape=(2,), dtype=int64)), 'labels': <tf.Tensor 'Placeholder_69:0' shape=(?, 2) dtype=float32>, 'labels_mask': <tf.Tensor 'Placeholder_70:0' shape=<unknown> dtype=int32>, 'dropout': <tf.Tensor 'PlaceholderWithDefault_17:0' shape=() dtype=float32>, 'num_features_nonzero': <tf.Tensor 'Placeholder_71:0' shape=<unknown> dtype=int32>}\n"
          ]
        }
      ],
      "source": [
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    # helper variable for sparse dropout\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
        "}\n",
        "\n",
        "print(placeholders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "4uEPq4s04wqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b603928-3061-4831-f8b2-94b73ec46b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85413\n",
            "Tensor(\"graphconvolution_2_8/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch: 0001 train_loss= 0.61910 train_acc= 0.48092 time= 12.76749\n",
            "Epoch: 0002 train_loss= 0.60318 train_acc= 0.73282 time= 11.63542\n",
            "Epoch: 0003 train_loss= 0.56767 train_acc= 0.70992 time= 11.69183\n",
            "Epoch: 0004 train_loss= 0.51449 train_acc= 0.73282 time= 11.70606\n",
            "Epoch: 0005 train_loss= 0.44955 train_acc= 0.74046 time= 11.40795\n",
            "Epoch: 0006 train_loss= 0.37612 train_acc= 0.79389 time= 11.75368\n",
            "Epoch: 0007 train_loss= 0.30037 train_acc= 0.89313 time= 11.81961\n",
            "Epoch: 0008 train_loss= 0.22828 train_acc= 0.89313 time= 11.94403\n",
            "Epoch: 0009 train_loss= 0.16745 train_acc= 0.93893 time= 11.78917\n",
            "Epoch: 0010 train_loss= 0.12120 train_acc= 0.96947 time= 12.30199\n",
            "Epoch: 0011 train_loss= 0.08511 train_acc= 0.97710 time= 12.23177\n",
            "Epoch: 0012 train_loss= 0.05828 train_acc= 0.98473 time= 11.75568\n",
            "Optimization Finished!\n"
          ]
        }
      ],
      "source": [
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i]\n",
        "                      for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict\n",
        "\n",
        "# Create model\n",
        "print(features[2][1])\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "sess = tf.Session(config=session_conf)\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(\n",
        "        features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(\n",
        "        features, support, y, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n",
        "                     model.layers[0].embedding], feed_dict=feed_dict)\n",
        "\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "print(\"Optimization Finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzphSnXJRYIM"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "aqgXt93kRgGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b441f8-263f-44ee-a482-aae7fa2617c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set results: cost= 0.55467 accuracy= 0.45614 time= 5.90067\n",
            "85413\n",
            "Test Precision, Recall and F1-Score...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.5161    0.5079        31\n",
            "           1     0.4000    0.3846    0.3922        26\n",
            "\n",
            "    accuracy                         0.4561        57\n",
            "   macro avg     0.4500    0.4504    0.4500        57\n",
            "weighted avg     0.4544    0.4561    0.4551        57\n",
            "\n",
            "Macro average Test Precision, Recall and F1-Score...\n",
            "(0.45, 0.45037220843672454, 0.45004668534080294, None)\n",
            "Micro average Test Precision, Recall and F1-Score...\n",
            "(0.45614035087719296, 0.45614035087719296, 0.45614035087719296, None)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Testing\n",
        "test_cost, test_acc, pred, labels, test_duration = evaluate(\n",
        "    features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
        "\n",
        "test_pred = []\n",
        "test_labels = []\n",
        "print(len(test_mask))\n",
        "for i in range(len(test_mask)):\n",
        "    if test_mask[i]:\n",
        "        test_pred.append(pred[i])\n",
        "        test_labels.append(labels[i])\n",
        "\n",
        "print(\"Test Precision, Recall and F1-Score...\")\n",
        "print(metrics.classification_report(test_labels, test_pred, digits=4))\n",
        "print(\"Macro average Test Precision, Recall and F1-Score...\")\n",
        "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
        "print(\"Micro average Test Precision, Recall and F1-Score...\")\n",
        "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgOZvaOZHaO3"
      },
      "execution_count": 215,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}