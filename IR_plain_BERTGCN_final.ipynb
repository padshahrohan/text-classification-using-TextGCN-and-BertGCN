{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgeXRA4Vd3OO"
      },
      "source": [
        "# Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgR2TwFZ6QIX",
        "outputId": "1ac5442d-cbfa-489c-c42f-47964abe3cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rmYNkZokXbk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNYmVQlhd8wo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Change the directory path to point where the data set file is located\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/PubMed_records_for_covid-19_labelled&unlabelled.xlsx - Sheet1 (1).csv\")\n",
        "\n",
        "df = df[['Article title','Article keywords', 'Article abstract', 'Contextual']].copy()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPyHpNI0fV1u"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs6MJiW1jJ4M"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "print(stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoNjcv2kjx5u"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "df['Article title'] = df['Article title'].str.lower()\n",
        "df['Article keywords'] = df['Article keywords'].str.lower()\n",
        "df['Article abstract'] = df['Article abstract'].str.lower()\n",
        "\n",
        "df['Article title'] = df['Article title'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
        "df['Article abstract'] = df['Article abstract'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
        "df['Article keywords'] = df['Article keywords'].str.replace('[{}]'.format(string.punctuation.replace('-','')), '')\n",
        "\n",
        "df['Article title'] = df['Article title'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "df['Article keywords'] = df['Article keywords'].apply(lambda x: ' '.join([item for item in str(x).split(';') if item not in stop]))\n",
        "df['Article abstract'] = df['Article abstract'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxDhqnkBkkVv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_labeled = df[df['Contextual'].notnull()]\n",
        "df_unlabeled = df[df['Contextual'].isnull()]\n",
        "df_train, df_test = train_test_split(df_labeled, test_size=0.2, shuffle=False)\n",
        "df_train, df_validation = train_test_split(df_train, test_size=0.06, shuffle=False)\n",
        "\n",
        "labeled_index_in_train = list(df_train.index.values)\n",
        "labeled_index_in_validation = list(df_validation.index.values)\n",
        "labeled_index_in_test = list(df_test.index.values)\n",
        "unlabeled_index = list(df_unlabeled.index.values)\n",
        "\n",
        "train_size = len(df_train) + len(df_validation) + len(df_test) + len(df_unlabeled)\n",
        "\n",
        "df_test_without_contextual = df_test[['Article title', 'Article keywords', 'Article abstract']]\n",
        "\n",
        "df_train = pd.concat([df_train, df_validation, df_test_without_contextual , df_unlabeled])\n",
        "\n",
        "# print(df_train.head())\n",
        "# print(df_test.head())\n",
        "\n",
        "print(labeled_index_in_train)\n",
        "print(labeled_index_in_validation)\n",
        "print(labeled_index_in_test)\n",
        "\n",
        "print(df_train.info())\n",
        "print(df_validation.info())\n",
        "print(df_test.info())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvslpG2Vmsck"
      },
      "source": [
        "# Build Graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQoGU2dLibzY"
      },
      "source": [
        "## Build list of training documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KuUv21xUooH"
      },
      "outputs": [],
      "source": [
        "row = []\n",
        "col = []\n",
        "weight = []\n",
        "\n",
        "train_list = []\n",
        "\n",
        "for index in df_train.index:\n",
        "  train_list.append(str(index) + ':=:' + df_train['Article title'][index] + df_train['Article keywords'][index] + df_train['Article abstract'][index]) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkkiiQ_V3yc1"
      },
      "source": [
        "## TF-IDF for document-word weight in graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fX51_3AkuOx"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "# doc word frequency\n",
        "# TF is simple raw frequency unlike (1+log(tf)), we might want to change that if accuracy is not good\n",
        "\n",
        "# build vocab\n",
        "word_set = set()\n",
        "for doc_words in train_list:\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    for word in words:\n",
        "        word_set.add(word)\n",
        "\n",
        "vocab = list(word_set)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_id_map = {}\n",
        "for i in range(vocab_size):\n",
        "    word_id_map[vocab[i]] = i\n",
        "\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(train_size):\n",
        "    doc_words = train_list[doc_id]\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1\n",
        "\n",
        "\n",
        "word_doc_list = {}\n",
        "\n",
        "for i in range(train_size):\n",
        "    doc_words = train_list[i]\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    appeared = set()\n",
        "    for word in words:\n",
        "        if word in appeared:\n",
        "            continue\n",
        "        if word in word_doc_list:\n",
        "            doc_list = word_doc_list[word]\n",
        "            doc_list.append(i)\n",
        "            word_doc_list[word] = doc_list\n",
        "        else:\n",
        "            word_doc_list[word] = [i]\n",
        "        appeared.add(word)\n",
        "\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "for i in range(train_size):\n",
        "    doc_words = train_list[i]\n",
        "    # words = doc_words.split()\n",
        "    row_index = int(doc_words.split(':=:')[0])\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        # row.append(i)\n",
        "        row.append(row_index)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(train_list) /\n",
        "                  word_doc_freq[vocab[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)\n",
        "\n",
        "print(len(row))\n",
        "print(len(col))\n",
        "print(len(weight))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69odkc3-yl84"
      },
      "source": [
        "## PMI Calculation for word-word edge weight in graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rurMveZjoL7U"
      },
      "outputs": [],
      "source": [
        "#Get the train as a list and define window size\n",
        "window_size = 20\n",
        "windows = []\n",
        "\n",
        "for doc_words in train_list:\n",
        "    # words = doc_words.split()\n",
        "    words = doc_words.split(':=:')[1].split()\n",
        "    length = len(words)\n",
        "    if length <= window_size:\n",
        "        windows.append(words)\n",
        "    else:\n",
        "        for j in range(length - window_size + 1):\n",
        "            window = words[j: j + window_size]\n",
        "            windows.append(window)\n",
        "\n",
        "# print(windows)\n",
        "\n",
        "word_window_freq = {}\n",
        "for window in windows:\n",
        "    appeared = set()\n",
        "    for i in range(len(window)):\n",
        "        if window[i] in appeared:\n",
        "            continue\n",
        "        if window[i] in word_window_freq:\n",
        "            word_window_freq[window[i]] += 1\n",
        "        else:\n",
        "            word_window_freq[window[i]] = 1\n",
        "        appeared.add(window[i])\n",
        "\n",
        "# print(word_window_freq)\n",
        "\n",
        "word_pair_count = {}\n",
        "for window in windows:\n",
        "    for i in range(1, len(window)):\n",
        "        for j in range(0, i):\n",
        "            word_i = window[i]\n",
        "            word_i_id = word_id_map[word_i]\n",
        "            word_j = window[j]\n",
        "            word_j_id = word_id_map[word_j]\n",
        "            if word_i_id == word_j_id:\n",
        "                continue\n",
        "            # print(word_i + ' ' + word_j + ' ' + str(word_i_id) + ' ' + str(word_j_id))\n",
        "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "            # two orders\n",
        "            # print(word_j + ' ' + word_i + ' ' + str(word_j_id) + ' ' + str(word_i_id))\n",
        "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "\n",
        "\n",
        "# print(word_pair_count)\n",
        "\n",
        "# pmi as weights\n",
        "\n",
        "num_window = len(windows)\n",
        "\n",
        "for key in word_pair_count:\n",
        "    temp = key.split(',')\n",
        "    i = int(temp[0])\n",
        "    j = int(temp[1])\n",
        "    count = word_pair_count[key]\n",
        "    word_freq_i = word_window_freq[vocab[i]]\n",
        "    word_freq_j = word_window_freq[vocab[j]]\n",
        "    pmi = log((1.0 * count / num_window) /\n",
        "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
        "    if pmi <= 0:\n",
        "        continue\n",
        "    #Adjust the position of pmi weights in final adjacency matrix\n",
        "    row.append(train_size + i)\n",
        "    col.append(train_size + j)\n",
        "    weight.append(pmi)\n",
        "\n",
        "print(len(row))\n",
        "print(len(col))\n",
        "print(len(weight))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjN00HkjUiXG"
      },
      "source": [
        "## Diagonal edge weight initialized to 1 in the adjacency matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcDQOk8vigxE"
      },
      "outputs": [],
      "source": [
        "node_size = train_size + vocab_size\n",
        "\n",
        "for i in range(node_size):\n",
        "  row.append(i)\n",
        "  col.append(i)\n",
        "  weight.append(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jynF9NAC5M8o"
      },
      "source": [
        "## Adjacency Matrix : A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bJPpbyPxFAe"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "\n",
        "#Train size contains all data (train + test both labeled and unlabeled)\n",
        "print(len(row))\n",
        "print(len(col)) \n",
        "print(len(weight))\n",
        "print(node_size)\n",
        "\n",
        "adj = sp.csr_matrix(\n",
        "    (weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "print(adj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnMhaEc3zdPW"
      },
      "source": [
        "## Build Feature Matrix: X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR9CGAfVIbuB"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "row_x = []\n",
        "col_x = []\n",
        "weight_x = []\n",
        "\n",
        "# One hot vector for X as per text GCN\n",
        "for i in range(node_size):\n",
        "  row_x.append(i)\n",
        "  col_x.append(i)\n",
        "  weight_x.append(1)\n",
        "\n",
        "x = sp.csr_matrix(\n",
        "    (weight_x, (row_x, col_x)), shape=(node_size, node_size))\n",
        "\n",
        "\n",
        "# x = sp.csr_matrix(\n",
        "#     (weight_x, (row_x, col_x)), shape=(node_size, 768))\n",
        "\n",
        "print(x.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8vobR5fav3Z"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VLofG4ojQfs"
      },
      "source": [
        "# BertGCN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGj83sbCIcFi"
      },
      "source": [
        "## Build label matrix: Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym_UTEjQpQOa"
      },
      "outputs": [],
      "source": [
        "train_y = []\n",
        "validation_y = []\n",
        "test_y = []\n",
        "unlabeled_y = []\n",
        "label_list = [0,1]\n",
        "\n",
        "for index in df_train.index:\n",
        "  if index in labeled_index_in_train:\n",
        "    label = int(df_train['Contextual'][index])\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    train_y.append(one_hot)\n",
        "    validation_y.append([0,0])\n",
        "    test_y.append([0,0])\n",
        "  elif index in labeled_index_in_validation:\n",
        "    label = int(df_train['Contextual'][index])\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    train_y.append([0,0])\n",
        "    validation_y.append(one_hot)\n",
        "    test_y.append([0,0])\n",
        "  elif index in labeled_index_in_test:\n",
        "    label = int(df_test['Contextual'][index])\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    train_y.append([0,0])\n",
        "    validation_y.append([0,0])\n",
        "    test_y.append(one_hot)\n",
        "  else:\n",
        "    train_y.append([0,0])\n",
        "    validation_y.append([0,0])\n",
        "    test_y.append([0,0])\n",
        "  unlabeled_y.append([0,0])\n",
        "\n",
        "for i in range(vocab_size):\n",
        "  train_y.append([0,0])\n",
        "  validation_y.append([0,0])\n",
        "  test_y.append([0,0])\n",
        "  unlabeled_y.append([0,0])\n",
        "\n",
        "train_y = np.array(train_y)\n",
        "validation_y = np.array(validation_y)\n",
        "test_y = np.array(test_y)\n",
        "unlabeled_y = np.array(unlabeled_y)\n",
        "\n",
        "# print(x)\n",
        "# print(y[:200])\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "features = sp.vstack((x)).tolil()\n",
        "features = sp.identity(features.shape[0])  \n",
        "print(features.shape)\n",
        "\n",
        "idx_train = labeled_index_in_train\n",
        "idx_val = labeled_index_in_validation\n",
        "idx_test = labeled_index_in_test\n",
        "idx_unlabeled = unlabeled_index\n",
        "\n",
        "print(idx_train)\n",
        "print(idx_val)\n",
        "print(idx_test)\n",
        "print(idx_unlabeled)\n",
        "\n",
        "train_mask = sample_mask(idx_train, train_y.shape[0])\n",
        "val_mask = sample_mask(idx_val, validation_y.shape[0])\n",
        "test_mask = sample_mask(idx_test, test_y.shape[0])\n",
        "unlabeled_mask = sample_mask(idx_unlabeled, unlabeled_y.shape[0])\n",
        "\n",
        "print(train_mask)\n",
        "print(val_mask)\n",
        "print(test_mask)\n",
        "print(unlabeled_mask)\n",
        "\n",
        "y_train = np.zeros(train_y.shape)\n",
        "y_val = np.zeros(validation_y.shape)\n",
        "y_test = np.zeros(test_y.shape)\n",
        "\n",
        "y_train[train_mask, :] = train_y[train_mask, :]\n",
        "y_val[val_mask, :] = validation_y[val_mask, :]\n",
        "y_test[test_mask, :] = test_y[test_mask, :]\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ8mi_XxQunY"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "\n",
        "max_length = 128\n",
        "batch_size = 80\n",
        "nb_epochs = 15\n",
        "bert_lr = 1e-4\n",
        "dataset = 'pubmed'\n",
        "bert_init = 'roberta-base'\n",
        "m = 0.7\n",
        "gcn_layers = 2\n",
        "n_hidden = 200\n",
        "heads = 8\n",
        "dropout = 0.5\n",
        "gcn_lr = 1e-3\n",
        "bert_lr = 1e-5\n",
        "\n",
        "\n",
        "cpu = th.device('cpu')\n",
        "gpu = th.device('cuda:0')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwDlF8GGRGq0"
      },
      "outputs": [],
      "source": [
        "nb_node = adj.shape[0]\n",
        "nb_train, nb_val, nb_test = train_mask.sum(), val_mask.sum(), test_mask.sum()\n",
        "nb_word = nb_node - nb_train - nb_val - nb_test - len(unlabeled_index)\n",
        "nb_class = y_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nELsgO_oRLg0"
      },
      "source": [
        "## Graph Convolutional Layer, GCN, BertGCN Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bcFlQAvuX1p"
      },
      "outputs": [],
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n",
        "!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g1Bnk1eROIU"
      },
      "outputs": [],
      "source": [
        "from dgl.nn.pytorch import GraphConv\n",
        "from dgl import function as fn\n",
        "from dgl.base import DGLError\n",
        "from dgl.utils import expand_as_pair\n",
        "\n",
        "class GraphConvEdgeWeight(GraphConv):\n",
        "\n",
        "    def forward(self, graph, feat,  weight=None, edge_weight=None):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n",
        "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
        "            if self._norm == 'both':\n",
        "                degs = graph.out_degrees().float().clamp(min=1)\n",
        "                norm = th.pow(degs, -0.5)\n",
        "                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                feat_src = feat_src * norm\n",
        "\n",
        "            if weight is not None:\n",
        "                if self.weight is not None:\n",
        "                    raise DGLError('External weight is provided while at the same time the'\n",
        "                                   ' module has defined its own weight parameter. Please'\n",
        "                                   ' create the module with flag weight=False.')\n",
        "            else:\n",
        "                weight = self.weight\n",
        "\n",
        "            if self._in_feats > self._out_feats:\n",
        "                # mult W first to reduce the feature size for aggregation.\n",
        "                if weight is not None:\n",
        "                    feat_src = th.matmul(feat_src, weight)\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                if edge_weight is None:\n",
        "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                else:\n",
        "                    graph.edata['a'] = edge_weight\n",
        "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "            else:\n",
        "                # aggregate first then mult W\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                if edge_weight is None:\n",
        "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                else:\n",
        "                    graph.edata['a'] = edge_weight\n",
        "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "                if weight is not None:\n",
        "                    rst = th.matmul(rst, weight)\n",
        "\n",
        "            if self._norm != 'none':\n",
        "                degs = graph.in_degrees().float().clamp(min=1)\n",
        "                if self._norm == 'both':\n",
        "                    norm = th.pow(degs, -0.5)\n",
        "                else:\n",
        "                    norm = 1.0 / degs\n",
        "                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                rst = rst * norm\n",
        "\n",
        "            if self.bias is not None:\n",
        "                rst = rst + self.bias\n",
        "\n",
        "            if self._activation is not None:\n",
        "                rst = self._activation(rst)\n",
        "\n",
        "            return rst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4vDlA-beRu2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 n_hidden,\n",
        "                 n_classes,\n",
        "                 n_layers,\n",
        "                 activation,\n",
        "                 dropout,\n",
        "                 normalization='none'):\n",
        "        super(GCN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        # input layer\n",
        "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation, norm=normalization))\n",
        "        # hidden layers\n",
        "        for i in range(n_layers - 1):\n",
        "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation, norm=normalization))\n",
        "        # output layer\n",
        "        self.layers.append(GraphConv(n_hidden, n_classes, norm=normalization))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, features, g, edge_weight):\n",
        "        h = features\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i != 0:\n",
        "                h = self.dropout(h)\n",
        "            h = layer(g, h, edge_weight=edge_weight)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPbPBS0UeSqx"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class BertGCN(th.nn.Module):\n",
        "    def __init__(self, pretrained_model='roberta_base', nb_class=20, m=0.7, gcn_layers=2, n_hidden=200, dropout=0.5):\n",
        "        super(BertGCN, self).__init__()\n",
        "        self.m = m\n",
        "        self.nb_class = nb_class\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
        "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
        "        self.classifier = th.nn.Linear(self.feat_dim, nb_class)\n",
        "        self.gcn = GCN(\n",
        "            in_feats=self.feat_dim,\n",
        "            n_hidden=n_hidden,\n",
        "            n_classes=nb_class,\n",
        "            n_layers=gcn_layers-1,\n",
        "            activation=F.elu,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, g, idx):\n",
        "        input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
        "        if self.training:\n",
        "            cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
        "            g.ndata['cls_feats'][idx] = cls_feats\n",
        "        else:\n",
        "            cls_feats = g.ndata['cls_feats'][idx]\n",
        "        cls_logit = self.classifier(cls_feats)\n",
        "        cls_pred = th.nn.Softmax(dim=1)(cls_logit)\n",
        "        gcn_logit = self.gcn(g.ndata['cls_feats'], g, g.edata['edge_weight'])[idx]\n",
        "        gcn_pred = th.nn.Softmax(dim=1)(gcn_logit)\n",
        "        pred = (gcn_pred+1e-10) * self.m + cls_pred * (1 - self.m)\n",
        "        pred = th.log(pred)\n",
        "        return pred\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcMkf1ivRTGf"
      },
      "outputs": [],
      "source": [
        "# instantiate model according to class number\n",
        "model = BertGCN(nb_class=nb_class, pretrained_model=bert_init, m=m, gcn_layers=gcn_layers,\n",
        "                    n_hidden=n_hidden, dropout=dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e78rGQvwRVaE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def encode_input(text, tokenizer):\n",
        "    input = tokenizer(text, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    return input.input_ids, input.attention_mask\n",
        "\n",
        "input_ids_, attention_mask_ = encode_input(train_list, model.tokenizer)\n",
        "\n",
        "# create train/test/val datasets and dataloaders\n",
        "input_ids_list = {'train':[], 'val': [], 'test': []}\n",
        "attention_mask_list = {'train':[], 'val': [], 'test': []}\n",
        "label_list = {'train':[], 'val': [], 'test': []}\n",
        "\n",
        "print(input_ids_.shape)\n",
        "for i in range(train_size):\n",
        "  if i in labeled_index_in_train:\n",
        "    input_ids_list['train'].append(input_ids_[i].tolist())\n",
        "    attention_mask_list['train'].append(attention_mask_[i].tolist())\n",
        "    label_list['train'].append(y_train[i].tolist())\n",
        "  elif i in labeled_index_in_validation:\n",
        "    input_ids_list['val'].append(input_ids_[i].tolist())\n",
        "    attention_mask_list['val'].append(attention_mask_[i].tolist())\n",
        "    label_list['val'].append(y_val[i].tolist())\n",
        "  elif i in labeled_index_in_test:\n",
        "    input_ids_list['test'].append(input_ids_[i].tolist())\n",
        "    attention_mask_list['test'].append(attention_mask_[i].tolist())\n",
        "    label_list['test'].append(y_test[i].tolist())\n",
        "  else:\n",
        "    input_ids_list['train'].append(input_ids_[i].tolist())\n",
        "    attention_mask_list['train'].append(attention_mask_[i].tolist())\n",
        "    label_list['train'].append(y_train[i].tolist())\n",
        "\n",
        "input_ids, attention_mask, label = {}, {}, {}\n",
        "input_ids['train'] = torch.tensor(input_ids_list['train'])\n",
        "input_ids['val'] = torch.tensor(input_ids_list['val'])\n",
        "input_ids['test'] = torch.tensor(input_ids_list['test'])\n",
        "\n",
        "attention_mask['train'] = torch.tensor(attention_mask_list['train'])\n",
        "attention_mask['val'] = torch.tensor(attention_mask_list['val'])\n",
        "attention_mask['test'] = torch.tensor(attention_mask_list['test'])\n",
        "\n",
        "input_ids = th.cat([input_ids['train'], input_ids['val'], input_ids['test'], th.zeros((nb_word, max_length), dtype=th.long)])\n",
        "attention_mask = th.cat([attention_mask['train'], attention_mask['val'], attention_mask['test'], th.zeros((nb_word, max_length), dtype=th.long)])\n",
        "\n",
        "# transform one-hot label to class ID for pytorch computation\n",
        "y = y_train + y_val + y_test \n",
        "y_train = y_train.argmax(axis=1)\n",
        "y = y.argmax(axis=1)\n",
        "\n",
        "# document mask used for update feature\n",
        "doc_mask  = train_mask + val_mask + test_mask + unlabeled_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7MwMQ_FVZa1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import dgl\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "# build DGL Graph\n",
        "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "print(adj_norm.shape)\n",
        "print(dgl.__version__)\n",
        "g = dgl.from_scipy(adj_norm.astype('float32'), eweight_name='edge_weight')\n",
        "print(input_ids.shape)\n",
        "print(attention_mask.shape)\n",
        "g.ndata['input_ids'], g.ndata['attention_mask'] = input_ids, attention_mask\n",
        "g.ndata['label'], g.ndata['train'], g.ndata['val'], g.ndata['test'] = \\\n",
        "    th.LongTensor(y), th.FloatTensor(train_mask), th.FloatTensor(val_mask), th.FloatTensor(test_mask)\n",
        "g.ndata['label_train'] = th.LongTensor(y_train)\n",
        "g.ndata['cls_feats'] = th.zeros((nb_node, model.feat_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S16ALyNrR4sf"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as Data\n",
        "\n",
        "# create index loader\n",
        "train_idx = Data.TensorDataset(torch.LongTensor(labeled_index_in_train))\n",
        "val_idx = Data.TensorDataset(torch.LongTensor(labeled_index_in_validation))\n",
        "test_idx = Data.TensorDataset(torch.LongTensor(labeled_index_in_test))\n",
        "unlabeled_idx = Data.TensorDataset(torch.LongTensor(unlabeled_index))\n",
        "doc_idx = Data.ConcatDataset([train_idx, val_idx, test_idx, unlabeled_idx])\n",
        "\n",
        "idx_loader_train = Data.DataLoader(train_idx, batch_size=batch_size, shuffle=True)\n",
        "idx_loader_val = Data.DataLoader(val_idx, batch_size=batch_size)\n",
        "idx_loader_test = Data.DataLoader(test_idx, batch_size=batch_size)\n",
        "idx_loader = Data.DataLoader(doc_idx, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPq4gMfuSEpx"
      },
      "source": [
        "## Train BertGCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyhTnxV0SG9S"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer, Engine\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "\n",
        "# Training\n",
        "def update_feature():\n",
        "    global model, g, doc_mask\n",
        "    # no gradient needed, uses a large batchsize to speed up the process\n",
        "    dataloader = Data.DataLoader(\n",
        "        Data.TensorDataset(g.ndata['input_ids'][doc_mask], g.ndata['attention_mask'][doc_mask]),\n",
        "        batch_size=256 #1024\n",
        "    )\n",
        "    with th.no_grad():\n",
        "        model = model.to(gpu)\n",
        "        model.eval()\n",
        "        cls_list = []\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            input_ids, attention_mask = [x.to(gpu) for x in batch]\n",
        "            output = model.bert_model(input_ids=input_ids, attention_mask=attention_mask)[0][:, 0]\n",
        "            cls_list.append(output.cpu())\n",
        "        cls_feat = th.cat(cls_list, axis=0)\n",
        "    g = g.to(cpu)\n",
        "    g.ndata['cls_feats'][doc_mask] = cls_feat\n",
        "    return g\n",
        "\n",
        "\n",
        "optimizer = th.optim.Adam([\n",
        "        {'params': model.bert_model.parameters(), 'lr': bert_lr},\n",
        "        {'params': model.classifier.parameters(), 'lr': bert_lr},\n",
        "        {'params': model.gcn.parameters(), 'lr': gcn_lr},\n",
        "    ], lr=1e-3\n",
        ")\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)\n",
        "\n",
        "\n",
        "def train_step(engine, batch):\n",
        "    global model, g, optimizer\n",
        "    model.train()\n",
        "    model = model.to(gpu)\n",
        "    g = g.to(gpu)\n",
        "    optimizer.zero_grad()\n",
        "    (idx, ) = [x.to(gpu) for x in batch]\n",
        "    optimizer.zero_grad()\n",
        "    train_mask = g.ndata['train'][idx].type(th.BoolTensor)\n",
        "    y_pred = model(g, idx)[train_mask]\n",
        "    y_true = g.ndata['label_train'][idx][train_mask]\n",
        "    loss = F.nll_loss(y_pred, y_true)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    g.ndata['cls_feats'].detach_()\n",
        "    train_loss = loss.item()\n",
        "    with th.no_grad():\n",
        "        if train_mask.sum() > 0:\n",
        "            y_true = y_true.detach().cpu()\n",
        "            y_pred = y_pred.argmax(axis=1).detach().cpu()\n",
        "            train_acc = accuracy_score(y_true, y_pred)\n",
        "        else:\n",
        "            train_acc = 1\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "trainer = Engine(train_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2zJscnWdbWi"
      },
      "outputs": [],
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def reset_graph(trainer):\n",
        "    scheduler.step()\n",
        "    update_feature()\n",
        "    th.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlg399CASJEX"
      },
      "source": [
        "## Test BertGCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsg-IeRTSLjf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_step(engine, batch):\n",
        "    global model, g\n",
        "    with th.no_grad():\n",
        "        model.eval()\n",
        "        model = model.to(gpu)\n",
        "        g = g.to(gpu)\n",
        "        (idx, ) = [x.to(gpu) for x in batch]\n",
        "        y_pred = model(g, idx)\n",
        "        y_true = g.ndata['label'][idx]\n",
        "        return y_pred, y_true\n",
        "\n",
        "\n",
        "evaluator = Engine(test_step)\n",
        "metrics={\n",
        "    'acc': Accuracy(),\n",
        "    'nll': Loss(th.nn.NLLLoss())\n",
        "}\n",
        "for n, f in metrics.items():\n",
        "    f.attach(evaluator, n)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REdjep7wdeph"
      },
      "outputs": [],
      "source": [
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_training_results(trainer):\n",
        "    evaluator.run(idx_loader_train)\n",
        "    metrics = evaluator.state.metrics\n",
        "    train_acc, train_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    evaluator.run(idx_loader_val)\n",
        "    metrics = evaluator.state.metrics\n",
        "    val_acc, val_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    print(\n",
        "        \"Epoch: {} Train acc: {:.4f} loss: {:.4f} Val acc: {:.4f} loss: {:.4f} \"\n",
        "        .format(trainer.state.epoch,train_acc, train_nll ,val_acc, val_nll)\n",
        "    )\n",
        "    if val_acc > log_training_results.best_val_acc:\n",
        "        log_training_results.best_val_acc = val_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPIHuXa0MDYk"
      },
      "outputs": [],
      "source": [
        "log_training_results.best_val_acc = 0\n",
        "g = update_feature()\n",
        "trainer.run(idx_loader, max_epochs=nb_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.run(idx_loader_test)\n",
        "metrics = evaluator.state.metrics\n",
        "test_acc, test_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "\n",
        "print(\n",
        "        \"Test acc: {:.4f} loss: {:.4f}\"\n",
        "        .format(test_acc, test_nll)\n",
        "    )"
      ],
      "metadata": {
        "id": "RP7O6lwHuDVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcZNrPBGbPQx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}